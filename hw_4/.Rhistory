prefex = "https://graph.facebook.com/v2.12/Starbucks?fields=posts%2Clikes&access_token="
url02    = paste0(prefex, token02)
res02    = httr::GET(url)
posts02  = data.frame(content(res02))
posts02  = content(res02)
token02="EAACEdEose0cBAEC8K44LuMZAl7EVZCFznZBU8ZB9J4YlaOoprpclsO9pd6tc8mbN4kL0UV1IBj3pWJWxF0aFyb3Y5bmZC7F7Ub5A6fQNBqXspDYN3NXfV2aDC5iqSkw71sXiHSkZBnVsiPVZCUtN7nuZAsQcgGveCPrnODZA66Gzl9IqZATVyKKMWQH9Dj2QSguaAZD"
prefex = "https://graph.facebook.com/v2.12/Starbucks?fields=posts%2Clikes&access_token="
url02    = paste0(prefex, token02)
res02    = httr::GET(url02)
posts02  = data.frame(content(res02))
token02="EAACEdEose0cBAEC8K44LuMZAl7EVZCFznZBU8ZB9J4YlaOoprpclsO9pd6tc8mbN4kL0UV1IBj3pWJWxF0aFyb3Y5bmZC7F7Ub5A6fQNBqXspDYN3NXfV2aDC5iqSkw71sXiHSkZBnVsiPVZCUtN7nuZAsQcgGveCPrnODZA66Gzl9IqZATVyKKMWQH9Dj2QSguaAZD"
prefex = "https://graph.facebook.com/v2.12/Starbucks?fields=posts%2Clikes&access_token="
url02    = paste0(prefex, token02)
res02    = httr::GET(url02)
posts02  = content(res02)
res02
library(httr)
token02="EAACEdEose0cBAEC8K44LuMZAl7EVZCFznZBU8ZB9J4YlaOoprpclsO9pd6tc8mbN4kL0UV1IBj3pWJWxF0aFyb3Y5bmZC7F7Ub5A6fQNBqXspDYN3NXfV2aDC5iqSkw71sXiHSkZBnVsiPVZCUtN7nuZAsQcgGveCPrnODZA66Gzl9IqZATVyKKMWQH9Dj2QSguaAZD"
prefex = "https://graph.facebook.com/v2.12/Starbucks?fields=posts%2Clikes&access_token="
url02    = paste0(prefex, token02)
res02    = httr::GET(url02)
posts02  = content(res02)
library(httr)
token02="EAACEdEose0cBAEC8K44LuMZAl7EVZCFznZBU8ZB9J4YlaOoprpclsO9pd6tc8mbN4kL0UV1IBj3pWJWxF0aFyb3Y5bmZC7F7Ub5A6fQNBqXspDYN3NXfV2aDC5iqSkw71sXiHSkZBnVsiPVZCUtN7nuZAsQcgGveCPrnODZA66Gzl9IqZATVyKKMWQH9Dj2QSguaAZD"
prefex = "https://graph.facebook.com/v2.12/Starbucks?fields=posts%2Clikes&access_token="
url02    = paste0(prefex, token02)
res02    = httr::GET(url02)
posts02  = content(res02)
library(httr)
token="EAACEdEose0cBAEC8K44LuMZAl7EVZCFznZBU8ZB9J4YlaOoprpclsO9pd6tc8mbN4kL0UV1IBj3pWJWxF0aFyb3Y5bmZC7F7Ub5A6fQNBqXspDYN3NXfV2aDC5iqSkw71sXiHSkZBnVsiPVZCUtN7nuZAsQcgGveCPrnODZA66Gzl9IqZATVyKKMWQH9Dj2QSguaAZD"
prefex = "https://graph.facebook.com/v2.12/Starbucks?fields=posts.limit(1000)%7Bmessage%7D&access_token=
"
url = paste0(prefex, token)
res = httr::GET(url)
posts = content(res)
res = httr::GET(url)
#--------------TRY AGAIN, collect posts only----------------#
library(httr)
token="EAACEdEose0cBAOOvgdSLcFQK0sXayME3lZBrbNvSGmFCpnZC6PeXg0pPP62eiHHDx0QKm6tGHqcKAZCNxeL0OE9eKSyJ1bPZAsS14Hd8uFYAS3wYAEq16JyCZBsBqQVkkt87pqZCCNjVHAkgiNYwUIKlkQANV1uTWnmjrWaNOXIUaEVCKbbifThRfUeWE6AkfcRQ17zBha5wZDZD"
prefex ="https://graph.facebook.com/v2.12/Starbucks?fields=posts.limit(1000)%7Bmessage%7D&access_token=
EAACEdEose0cBAOOvgd"
url = paste0(prefex, token)
res = httr::GET(url)
posts = content(res)
prefex ="https://graph.facebook.com/v2.12/22092443056?fields=posts%7Bmessage%7D&access_token="
library(httr)
token="EAACEdEose0cBAEW6jTIuizxW7qMx8qQoMiMhLRmO3hwzWzUfgrqjldK0ZAuNWN8gkfQZBVuviTfq9AgrCkhztDyNalABKu2gdH9kXcLFFdJAQ9AW2cDriTwJunGogfhj06ZCZBXZBft0rZAS5XKsKZBTE5lJKPa66EvBU65AsYq56t7hMOj9BjNP7TN7cPi0HLLQT7kEYJhhwZDZD"
prefex ="https://graph.facebook.com/v2.12/22092443056?fields=posts%7Bmessage%7D&access_token="
url = paste0(prefex, token)
res = httr::GET(url)
posts = content(res)
View(posts)
posts02= posts$posts$data$message
posts02= posts$posts$data
posts03= posts03[[-c(6,7)]]
posts03= posts02[[-c(6,7)]]
posts03= posts02[-c(6,7)]
class(posts03)
View(posts03)
posts04= data.frame(posts03)
View(posts04)
posts04= data.frame(posts03,byrow=TRUE,ncol=2)
posts04= matrix(posts03,byrow=FALSE,ncol=2)
posts04= matrix(posts03,byrow=TRUE,ncol=2)
posts_csv=read.csv(posts)
posts02= posts$posts$data
#notice that the 6th and 7th only have id but not message
posts03= posts02[-c(6,7)]
class(posts03)
posts04= data.frame(posts03)
View(posts04)
require(stats)
posts05<-stack(posts04)                    # now put it back together
posts05<-stack(posts04, select = -ctrl)    # omitting one vector
#posts05<-stack(posts04)                    # now put it back together
posts05<-stack(posts04, select = -ctrl)    # omitting one vector
#posts05<-stack(posts04)                    # now put it back together
posts05<-stack(posts04, select = -id)    # omitting one vector
#posts05<-stack(posts04)                    # now put it back together
posts05<-stack(posts04, select = -"id")    # omitting one vector
View(posts04)
#save as matrix first
post04<-matrix(posts03,byrow=FALSE,ncol=2)
View(posts04)
posts04= data.frame(posts03)
View(posts04)
posts04 <- data.frame(matrix(unlist(posts03), nrow=23, byrow=T))
View(posts04)
rm(post04)
row.names(posts04) <- c("message","id")
colnames(posts04) <- c("message","id")
View(posts04)
library(NLP)
library(tm)
library(RColorBrewer)
library(wordcloud)
str(posts04)
class(posts04)
View(posts04)
posts05<-posts04[,1]
posts05<-data.frame(posts04[,1])
View(posts05)
source('D:/GitHub/R-course-2018/course week_4/course_4/example_2_facebookApi_myprac_2.R')
posts06 <- tm_map(posts05, removePunctuation)
#first convert dataframe into VectorSource
posts06 <- Corpus(VectorSource(posts05))
View(posts06)
#first convert dataframe into VectorSource
posts06 <- Corpus(VectorSource(posts04))
#first convert dataframe into VectorSource
posts06 <- Corpus(VectorSource(posts05))
posts06 <- tm_map(posts06, removePunctuation)
posts06 <- tm_map(posts06, removePunctuation)
posts06 <- tm_map(posts06, removeNumbers)
posts06 <- tm_map(posts06, stripWhitespace)
source('D:/GitHub/R-course-2018/course week_4/course_4/example_2_facebookApi_myprac_2.R')
library(Rfacebook)
install.packages("Rfacebook")
library(NLP)
library(tm)
library(RColorBrewer)
library(wordcloud)
page.id<-"22092443056"
token<-"EAACEdEose0cBAJfDteCwPgpzALAfThhTGqRkA94p58WZCLtSwmth5cXGfn4OUfHa0TzHYa5Ea7sHCVtalqCfL9tpSv1btsfNe8YGpBFV8Tm5XMMqirj6hJ7aVUuQLIMowkeG6IpqINJm3n5JxWpC3JEijuqg93y1xfjye4JE6jH4v2LpHZAqYOEMOUDQnwfjPfZCIXF9wZDZD"
page<-getPage(page.id, token, n=300)
page<-getPage(page.id,token, n==2)
library(Rfacebook)
page<-getPage(page.id,token, n==2)
page<-getPage(page.id,token, n=2)
View(page)
page<-getPage(page.id,token, n=300)
View(page)
doc01<-Corpus(VectorSource(page$message))
View(doc01)
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
)
#remove punctuation and others
doc02<-tm_map(doc01,removePunctuation)
doc02<-tm_map(doc01,removePunctuation)
doc02<-tm_map(doc01,removeNumbers)
doc02<-tm_map(doc01,stripWhitespace)
View(doc02)
doc02<-tm_map(doc01,content_transformer(tolower))
#定義清洗：清洗就是把你找到的符號用空白取代
docs <- tm_map(docs, toSpace, "<U+00AF>")
#定義清洗：清洗就是把你找到的符號用空白取代
doc02<- tm_map(doc02, toSpace, "<U+00AF>")
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
)
#定義清洗：清洗就是把你找到的符號用空白取代
doc02<- tm_map(doc02, toSpace, "<U+00AF>")
doc02<- tm_map(doc02, toSpace, ">")
doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
#要清洗掉的東西
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
)
#定義清洗：清洗就是把你找到的符號用空白取代
doc02<- tm_map(doc02, toSpace, "<U+00AF>")
doc02<- tm_map(doc02, toSpace, ">")
doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
#remove punctuation and others
doc02<-tm_map(doc01,removePunctuation)
doc02<-tm_map(doc01,removeNumbers)
doc02<-tm_map(doc01,stripWhitespace)
doc02<-tm_map(doc01,content_transformer(tolower))
View(doc02)
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
View(doc01)
doc02 <- gsub("[^0-9A-Za-z///' ]","'" , doc01 ,ignore.case = TRUE)
class(doc02)
source('D:/GitHub/R-course-2018/hw_4/hw_4_facebook.R', encoding = 'UTF-8')
source('D:/GitHub/R-course-2018/hw_4/hw_4_facebook.R', encoding = 'UTF-8')
inspect(doc01)
library(tm)
inspect(doc01)
source('D:/GitHub/R-course-2018/hw_4/hw_4_facebook.R', encoding = 'UTF-8')
rm(doc02)
View(doc01)
source('D:/GitHub/R-course-2018/hw_4/hw_4_facebook.R', encoding = 'UTF-8')
inspect(doc01)
#定義清洗：清洗就是把你找到的符號用空白取代
doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc02, toSpace, ">")
doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
inspect(doc01[c(1:10)])
doc02<- tm_map(doc02,toSpace,"\xed��\")
doc02<- tm_map(doc02, toSpace,"\xed")
doc02<- tm_map(doc02, toSpace,"\")
doc02<- tm_map(doc02, toSpace,"\")
doc02<- tm_map(doc02, toSpace, "\")
doc02<- tm_map(doc02, toSpace, "\ ")
doc02<- tm_map(doc02, toSpace, "/")
doc02<- tm_map(doc02, toSpace, "@")
inspect(doc02[c(10:20)])
rm(doc02)
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#定義清洗：清洗就是把你找到的符號用空白取代
doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc02, toSpace, ">")
doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
doc02<- tm_map(doc02, toSpace, "/")
doc02<- tm_map(doc02, toSpace, "@")
#remove punctuation and others
doc02<-tm_map(doc01,removePunctuation)
doc02<-tm_map(doc01,removeNumbers)
doc02<-tm_map(doc01,stripWhitespace)
inspect(doc02[c(10:20)])
#remove punctuation and others
doc02<-tm_map(doc01,removePunctuation, ucp=TRUE)
inspect(doc02[c(10:20)])
doc02<-tm_map(doc01,removePunctuation, ucp=TRUE)
#ucp: a logical specifying whether to use Unicode character properties for determining punctuation characters. If FALSE (default), characters in the ASCII [:punct:] class are taken; if TRUE, the characters with Unicode general category P (Punctuation).
doc02<-tm_map(doc01,removeNumbers)
doc02<-tm_map(doc02,stripWhitespace)
doc02<-tm_map(doc02,content_transformer(tolower))
inspect(doc02[c(10:20)])
#定義清洗：清洗就是把你找到的符號用空白取代
doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc02, toSpace, ">")
doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
doc02<- tm_map(doc02, toSpace, "/")
doc02<- tm_map(doc02, toSpace, "@")
#remove punctuation and others
doc02<-tm_map(doc02,removePunctuation, ucp=TRUE)
#ucp: a logical specifying whether to use Unicode character properties for determining punctuation characters. If FALSE (default), characters in the ASCII [:punct:] class are taken; if TRUE, the characters with Unicode general category P (Punctuation).
doc02<-tm_map(doc01,removeNumbers)
doc02<-tm_map(doc02,stripWhitespace)
doc02<-tm_map(doc02,content_transformer(tolower))
inspect(doc02[c(10:20)])
rm(doc02)
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#定義清洗：清洗就是把你找到的符號用空白取代
doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc02, toSpace, ">")
doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
doc02<- tm_map(doc02, toSpace, "/")
doc02<- tm_map(doc02, toSpace, "@")
#remove punctuation and others
doc02<-tm_map(doc02,removePunctuation, ucp=TRUE)
#ucp: a logical specifying whether to use Unicode character properties for determining punctuation characters. If FALSE (default), characters in the ASCII [:punct:] class are taken; if TRUE, the characters with Unicode general category P (Punctuation).
doc02<-tm_map(doc02,removeNumbers)
doc02<-tm_map(doc02,stripWhitespace)
doc02<-tm_map(doc02,content_transformer(tolower))
inspect(doc02[c(10:20)])
doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc02, toSpace, ">")
doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
doc02<- tm_map(doc02, toSpace, "/")
doc02<- tm_map(doc02, toSpace, "@")
#remove punctuation and others
doc02<-tm_map(doc02,removePunctuation)
#ucp: a logical specifying whether to use Unicode character properties for determining punctuation characters. If FALSE (default), characters in the ASCII [:punct:] class are taken; if TRUE, the characters with Unicode general category P (Punctuation).
doc02<-tm_map(doc02,removeNumbers)
doc02<-tm_map(doc02,stripWhitespace)
doc02<-tm_map(doc02,content_transformer(tolower))
inspect(doc02[c(10:20)])
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#定義清洗：清洗就是把你找到的符號用空白取代
#doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc02, toSpace, ">")
doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
doc02<- tm_map(doc02, toSpace, "/")
doc02<- tm_map(doc02, toSpace, "@")
#remove punctuation and others
doc02<-tm_map(doc02,removePunctuation)
#ucp: a logical specifying whether to use Unicode character properties for determining punctuation characters. If FALSE (default), characters in the ASCII [:punct:] class are taken; if TRUE, the characters with Unicode general category P (Punctuation).
doc02<-tm_map(doc02,removeNumbers)
doc02<-tm_map(doc02,stripWhitespace)
doc02<-tm_map(doc02,content_transformer(tolower))
inspect(doc02[c(10:20)])
rm(doc02)
#要清洗掉的東西
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#定義清洗：清洗就是把你找到的符號用空白取代
#doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc02, toSpace, ">")
doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
doc02<- tm_map(doc02, toSpace, "/")
doc02<- tm_map(doc02, toSpace, "@")
#remove punctuation and others
doc02<-tm_map(doc02,removePunctuation)
#ucp: a logical specifying whether to use Unicode character properties for determining punctuation characters. If FALSE (default), characters in the ASCII [:punct:] class are taken; if TRUE, the characters with Unicode general category P (Punctuation).
doc02<-tm_map(doc02,removeNumbers)
doc02<-tm_map(doc02,stripWhitespace)
#doc02<-tm_map(doc02,content_transformer(tolower))
inspect(doc02[c(10:20)])
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#定義清洗：清洗就是把你找到的符號用空白取代
doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc02, toSpace, ">")
doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
doc02<- tm_map(doc02, toSpace, "/")
doc02<- tm_map(doc02, toSpace, "@")
inspect(doc02[c(1:10)])
rm(doc02)
#要清洗掉的東西
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#定義清洗：清洗就是把你找到的符號用空白取代
#doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc02, toSpace, ">")
#doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
doc02<- tm_map(doc02, toSpace, "/")
doc02<- tm_map(doc02, toSpace, "@")
#要清洗掉的東西
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#定義清洗：清洗就是把你找到的符號用空白取代
#doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc01, toSpace, ">")
#doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
doc02<- tm_map(doc02, toSpace, "/")
doc02<- tm_map(doc02, toSpace, "@")
inspect(doc02[c(1:10)])
inspect(doc02[c(1:10)])
library(Rfacebook)
library(NLP)
library(tm)
library(RColorBrewer)
library(wordcloud)
inspect(doc02[c(1:10)])
class(doc02)
inspect(doc01[c(1:10)])
#要清洗掉的東西
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#定義清洗：清洗就是把你找到的符號用空白取代
doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc02, toSpace, ">")
doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
#remove punctuation and others
doc02<-tm_map(doc01,removePunctuation)
doc02<-tm_map(doc01,removeNumbers)
doc02<-tm_map(doc01,stripWhitespace)
inspect(doc02[c(1:10)])
#要清洗掉的東西
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#定義清洗：清洗就是把你找到的符號用空白取代
doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc02, toSpace, ">")
doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
doc02<- tm_map(doc02, toSpace, "#")
doc02<- tm_map(doc02, toSpace, "/")
doc02<- tm_map(doc02, toSpace, "@")
#remove punctuation and others
doc02<-tm_map(doc02,removePunctuation)
doc02<-tm_map(doc02,removeNumbers)
doc02<-tm_map(doc02,stripWhitespace)
inspect(doc02[c(1:10)])
#要清洗掉的東西
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#定義清洗：清洗就是把你找到的符號用空白取代
doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc02, toSpace, ">")
doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
doc02<- tm_map(doc02, toSpace, "#")
doc02<- tm_map(doc02, toSpace, "/")
doc02<- tm_map(doc02, toSpace, "@")
inspect(doc02[c(1:10)])
#要清洗掉的東西
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#定義清洗：清洗就是把你找到的符號用空白取代
doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc02, toSpace, ">")
doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
inspect(doc02[c(1:10)])
library(Rfacebook)
library(NLP)
library(tm)
library(RColorBrewer)
library(wordcloud)
#要清洗掉的東西
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#定義清洗：清洗就是把你找到的符號用空白取代
doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc02, toSpace, ">")
doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
inspect(doc02[c(1:10)])
View(doc02)
#remove punctuation and others
doc02<-tm_map(doc02,removePunctuation)
doc02<-tm_map(doc02,removeNumbers)
doc02<-tm_map(doc02,stripWhitespace)
inspect(doc02[c(1:10)])
rm(doc02)
#要清洗掉的東西
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#定義清洗：清洗就是把你找到的符號用空白取代
#doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc02, toSpace, ">")
#doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
inspect(doc02[c(1:10)])
#remove punctuation and others
doc02<-tm_map(doc02,removePunctuation)
doc02<-tm_map(doc02,removeNumbers)
doc02<-tm_map(doc02,stripWhitespace)
#要清洗掉的東西
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#定義清洗：清洗就是把你找到的符號用空白取代
#doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc01, toSpace, ">")
#doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
inspect(doc02[c(1:10)])
#remove punctuation and others
doc02<-tm_map(doc02,removePunctuation)
doc02<-tm_map(doc02,removeNumbers)
doc02<-tm_map(doc02,stripWhitespace)
inspect(doc02[c(1:10)])
rm(doc02)
#要清洗掉的東西
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#定義清洗：清洗就是把你找到的符號用空白取代
#doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc01, toSpace, ">")
#doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
inspect(doc02[c(1:10)])
#remove punctuation and others
doc02<-tm_map(doc02,removePunctuation,ucp=TRUE)
doc02<-tm_map(doc02,removeNumbers)
doc02<-tm_map(doc02,stripWhitespace)
inspect(doc02[c(1:10)])
rm(doc02)
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
doc02<- tm_map(doc01, toSpace, ">")
inspect(doc02[c(5:10)])
doc02<- tm_map(doc02, toSpace, "+")
inspect(doc02[c(5:10)])
inspect(doc02[c(5:10)])
source('D:/GitHub/R-course-2018/hw_4/hw_4_facebook.R', encoding = 'UTF-8')
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#定義清洗：清洗就是把你找到的符號用空白取代
#doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc01, toSpace, ">")
inspect(doc02[c(5:10)])
doc02<- tm_map(doc02, toSpace, "#")
inspect(doc02[c(5:10)])
doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc01, toSpace, ">")
doc02<- tm_map(doc02, toSpace, "<U+")
inspect(doc02[c(5:10)])
doc02<-tm_map(doc02,removePunctuation,ucp=TRUE)
doc02<-tm_map(doc02,removeNumbers)
doc02<-tm_map(doc02,stripWhitespace)
inspect(doc02[c(1:10)])
source('D:/GitHub/R-course-2018/hw_4/hw_4_facebook.R', encoding = 'UTF-8')
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#定義清洗：清洗就是把你找到的符號用空白取代
#doc02<- tm_map(doc01, toSpace, "<U+00AF>")
doc02<- tm_map(doc01, toSpace, ">")
inspect(doc02[c(5:10)])
#doc02<- tm_map(doc02, toSpace, "<U+")
##problem at this command
###doc02<- tm_map(doc02, toSpace, "+")
doc02<- tm_map(doc02, toSpace, "#")
inspect(doc02[c(5:10)])
doc02<- tm_map(doc02, toSpace, "<U+00AF>")
doc02<- tm_map(doc02, toSpace, ">")
doc02<- tm_map(doc02, toSpace, "<U+")
doc02<- tm_map(doc02, toSpace, "#")
doc02<- tm_map(doc02, toSpace, "/")
doc02<- tm_map(doc02, toSpace, "@")
inspect(doc02[c(5:10)])
doc02<-tm_map(doc02,removePunctuation,ucp=TRUE)
doc02<-tm_map(doc02,removeNumbers)
doc02<-tm_map(doc02,stripWhitespace)
inspect(doc02[c(1:10)])
doc02<-tm_map(doc02,content_transformer(tolower))
inspect(doc02[c(1:10)])
source('D:/GitHub/R-course-2018/hw_4/hw_4_facebook.R', encoding = 'UTF-8')
